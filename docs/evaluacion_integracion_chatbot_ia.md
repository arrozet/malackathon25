# EvaluaciÃ³n: IntegraciÃ³n de Chatbot IA con RAG en Brain

**Fecha:** 2025-10-15  
**Proyecto:** Brain - Malackathon 2025  
**Objetivo:** Evaluar la complejidad de integrar un chatbot conversacional con capacidades RAG, herramientas y acceso a la base de datos Oracle.

---

## ğŸ“Š Resumen Ejecutivo

**Veredicto:** **Complejidad MEDIA-ALTA** (6-8 dÃ­as de desarrollo full-time)

La integraciÃ³n de un chatbot con IA en Brain es **totalmente viable** dentro del contexto del hackathon, pero requiere decisiones arquitectÃ³nicas claras y una implementaciÃ³n metÃ³dica. Las principales ventajas son:

- âœ… Backend FastAPI ya establecido con conexiÃ³n estable a Oracle
- âœ… Frontend React modular que puede extenderse con componente chat
- âœ… Infraestructura Docker lista para nuevos servicios
- âœ… MÃºltiples opciones de LLM providers con diferentes trade-offs

**Puntos crÃ­ticos de riesgo:**
- âš ï¸ Latencia de consultas Oracle en contexto conversacional
- âš ï¸ GestiÃ³n de costes de APIs externas (OpenAI, OpenRouter)
- âš ï¸ Complejidad de implementar tool calling robusto
- âš ï¸ Embedding y vectorizaciÃ³n de esquema de BD grande

---

## ğŸ—ï¸ Arquitectura Propuesta

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FRONTEND (React + TS)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Componente ChatInterface                            â”‚   â”‚
â”‚  â”‚  - Input conversacional                              â”‚   â”‚
â”‚  â”‚  - Historial de mensajes                             â”‚   â”‚
â”‚  â”‚  - VisualizaciÃ³n de insights en tiempo real          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚ HTTPS
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   BACKEND (FastAPI)                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  /chat/message endpoint                              â”‚   â”‚
â”‚  â”‚  /chat/history endpoint                              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Orquestador de IA                                   â”‚   â”‚
â”‚  â”‚  - GestiÃ³n de contexto conversacional                â”‚   â”‚
â”‚  â”‚  - DecisiÃ³n de uso de herramientas (tool calling)    â”‚   â”‚
â”‚  â”‚  - Prompt engineering para anÃ¡lisis sanitario        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Motor RAG                                           â”‚   â”‚
â”‚  â”‚  - Embedding de esquema Oracle (columnas, tablas)    â”‚   â”‚
â”‚  â”‚  - BÃºsqueda semÃ¡ntica en metadatos                   â”‚   â”‚
â”‚  â”‚  - GeneraciÃ³n de consultas SQL desde lenguaje naturalâ”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Herramientas Disponibles (Tools)                    â”‚   â”‚
â”‚  â”‚  1. query_oracle_tool()                              â”‚   â”‚
â”‚  â”‚  2. calculate_statistics_tool()                      â”‚   â”‚
â”‚  â”‚  3. search_internet_tool() [opcional]                â”‚   â”‚
â”‚  â”‚  4. export_data_tool()                               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                    â”‚                    â”‚
         â–¼                    â–¼                    â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Oracle â”‚      â”‚ LLM Provider â”‚      â”‚ Vector DB  â”‚
    â”‚   ADB   â”‚      â”‚ (OpenAI/etc) â”‚      â”‚ (opcional) â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¤– Opciones de LLM Provider

### OpciÃ³n 1: OpenAI GPT-4 / GPT-4o
**Complejidad:** â­â­ (BAJA)

**Pros:**
- API extremadamente estable y documentada
- Excelente capacidad de function calling nativa
- Soporte robusto de system prompts para contexto mÃ©dico
- Latencia predecible (~1-3s por request)

**Contras:**
- Coste mÃ¡s alto (~$0.03-0.06 por 1K tokens en GPT-4o)
- Requiere API key con billing configurado
- Dependencia de infraestructura externa

**ImplementaciÃ³n:**
```python
from openai import AsyncOpenAI

client = AsyncOpenAI(api_key=config.OPENAI_API_KEY)

tools = [
    {
        "type": "function",
        "function": {
            "name": "query_oracle",
            "description": "Ejecuta una consulta SQL en la base de datos de admisiones de salud mental",
            "parameters": {
                "type": "object",
                "properties": {
                    "sql_query": {
                        "type": "string",
                        "description": "Consulta SQL parametrizada y segura"
                    }
                },
                "required": ["sql_query"]
            }
        }
    }
]

response = await client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {"role": "system", "content": MEDICAL_CONTEXT_PROMPT},
        {"role": "user", "content": user_message}
    ],
    tools=tools,
    tool_choice="auto"
)
```

**EstimaciÃ³n de esfuerzo:** 2-3 dÃ­as

---

### OpciÃ³n 2: OpenRouter (Agregador Multi-Modelo)
**Complejidad:** â­â­ (BAJA-MEDIA)

**Pros:**
- Acceso a mÃºltiples modelos (Claude, GPT-4, Gemini, Llama) con una API
- Precios competitivos y transparentes
- Fallback automÃ¡tico entre modelos
- Compatible con SDK de OpenAI

**Contras:**
- Capa adicional de abstracciÃ³n
- Soporte de function calling varÃ­a segÃºn modelo subyacente
- Menor control sobre infraestructura

**ImplementaciÃ³n:**
```python
from openai import AsyncOpenAI

# OpenRouter usa la misma API que OpenAI
client = AsyncOpenAI(
    api_key=config.OPENROUTER_API_KEY,
    base_url="https://openrouter.ai/api/v1"
)

response = await client.chat.completions.create(
    model="anthropic/claude-3.5-sonnet",  # O cualquier otro modelo
    messages=messages,
    extra_headers={
        "HTTP-Referer": "https://brain.malackathon.es",
        "X-Title": "Brain Assistant"
    }
)
```

**EstimaciÃ³n de esfuerzo:** 2-3 dÃ­as

---

### OpciÃ³n 3: xAI Grok
**Complejidad:** â­â­ (BAJA)

**Pros:**
- Modelo reciente y competitivo
- Pricing agresivo
- API compatible con OpenAI SDK

**Contras:**
- Menos maduro que OpenAI/Anthropic
- DocumentaciÃ³n menos exhaustiva
- Ecosistema de herramientas mÃ¡s limitado

**ImplementaciÃ³n:**
```python
from openai import AsyncOpenAI

client = AsyncOpenAI(
    api_key=config.XAI_API_KEY,
    base_url="https://api.x.ai/v1"
)

response = await client.chat.completions.create(
    model="grok-beta",
    messages=messages
)
```

**EstimaciÃ³n de esfuerzo:** 2-3 dÃ­as

---

### OpciÃ³n 4: Oracle Cloud Infrastructure Gen AI (Nativo)
**Complejidad:** â­â­â­â­ (ALTA)

**Pros:**
- IntegraciÃ³n nativa con Oracle Cloud
- Sin latencia de red externa
- Potencial de costes reducidos (misma facturaciÃ³n OCI)
- Cumplimiento normativo healthcare

**Contras:**
- DocumentaciÃ³n menos madura
- Requiere configuraciÃ³n adicional de OCI
- Ecosistema de herramientas limitado
- Mayor curva de aprendizaje

**ImplementaciÃ³n:**
```python
import oci

config_oci = oci.config.from_file()
generative_ai_client = oci.generative_ai_inference.GenerativeAiInferenceClient(config_oci)

chat_request = oci.generative_ai_inference.models.CohereChatRequest(
    message=user_message,
    chat_history=conversation_history
)

response = generative_ai_client.chat(chat_request)
```

**EstimaciÃ³n de esfuerzo:** 5-7 dÃ­as (por complejidad de setup)

---

## ğŸ§  ImplementaciÃ³n de RAG sobre Oracle

### Componentes Necesarios

#### 1. Embedding del Esquema de Base de Datos
**Complejidad:** â­â­â­ (MEDIA)

Necesitas vectorizar:
- Nombres de tablas y sus propÃ³sitos
- Nombres de columnas y tipos de datos
- Relaciones entre tablas
- Valores categÃ³ricos comunes (ej: cÃ³digos de diagnÃ³stico)

**CÃ³digo de ejemplo:**
```python
from openai import AsyncOpenAI
import numpy as np

async def embed_database_schema():
    """Genera embeddings del esquema Oracle para bÃºsqueda semÃ¡ntica."""
    
    schema_docs = [
        {
            "id": "table_saludmental",
            "text": "Tabla SALUDMENTAL: Contiene registros de admisiones hospitalarias en unidades de salud mental. Columnas: CIP_SNS_RECODIFICADO (identificador paciente), FECHA_DE_INGRESO, SEXO (1=Hombre, 2=Mujer), EDAD, Estancia DÃ­as, REINGRESO (S/N), CategorÃ­a (diagnÃ³stico primario), INGRESO_EN_UCI (S/N)"
        },
        {
            "id": "vista_muy_interesante",
            "text": "Vista VISTA_MUY_INTERESANTE: AgregaciÃ³n de mÃ©tricas clave por categorÃ­a diagnÃ³stica y grupo etario"
        }
    ]
    
    embeddings = []
    for doc in schema_docs:
        response = await client.embeddings.create(
            model="text-embedding-3-small",  # MÃ¡s barato que ada-002
            input=doc["text"]
        )
        embeddings.append({
            "id": doc["id"],
            "text": doc["text"],
            "embedding": response.data[0].embedding
        })
    
    return embeddings

# Almacena en memoria o Vector DB
schema_embeddings = await embed_database_schema()
```

**Storage de vectores:**
- **OpciÃ³n simple:** In-memory con NumPy/FAISS (suficiente para hackathon)
- **OpciÃ³n robusta:** Qdrant, Weaviate, Pinecone (overkill para hackathon)

#### 2. BÃºsqueda SemÃ¡ntica de Contexto Relevante
```python
async def find_relevant_context(user_query: str, top_k: int = 3):
    """Encuentra las tablas/columnas mÃ¡s relevantes para la query del usuario."""
    
    # Generar embedding de la pregunta
    query_response = await client.embeddings.create(
        model="text-embedding-3-small",
        input=user_query
    )
    query_embedding = query_response.data[0].embedding
    
    # Calcular similitud coseno
    similarities = []
    for doc in schema_embeddings:
        similarity = np.dot(query_embedding, doc["embedding"]) / (
            np.linalg.norm(query_embedding) * np.linalg.norm(doc["embedding"])
        )
        similarities.append((doc, similarity))
    
    # Retornar top-k mÃ¡s relevantes
    similarities.sort(key=lambda x: x[1], reverse=True)
    return [doc for doc, score in similarities[:top_k]]
```

#### 3. GeneraciÃ³n de SQL desde Lenguaje Natural
**Complejidad:** â­â­â­â­ (ALTA - ÃREA DE MAYOR RIESGO)

**DesafÃ­o:** Convertir "Â¿cuÃ¡ntas mujeres menores de 30 aÃ±os han reingresado?" en:
```sql
SELECT COUNT(*) 
FROM SALUDMENTAL 
WHERE SEXO = 2 
  AND EDAD < 30 
  AND REINGRESO = 'S'
```

**Estrategia:**
```python
async def generate_sql_from_natural_language(user_query: str):
    """Genera SQL seguro desde lenguaje natural con validaciÃ³n."""
    
    # 1. Recuperar contexto relevante
    relevant_context = await find_relevant_context(user_query)
    
    # 2. Construir prompt con ejemplos few-shot
    system_prompt = f"""Eres un experto en SQL y anÃ¡lisis de datos de salud mental.
    
Esquema de base de datos:
{chr(10).join([doc['text'] for doc, _ in relevant_context])}

Genera consultas SQL vÃ¡lidas para Oracle Database 23ai.
IMPORTANTE:
- Usa comillas dobles para columnas con espacios: "Estancia DÃ­as"
- SEXO: 1=Hombre, 2=Mujer
- REINGRESO: 'S'/'N'
- Siempre incluye WHERE para filtros
- NUNCA uses DELETE, DROP, TRUNCATE, UPDATE, INSERT

Ejemplos:
Usuario: "CuÃ¡ntos pacientes tienen mÃ¡s de 60 aÃ±os"
SQL: SELECT COUNT(*) FROM SALUDMENTAL WHERE EDAD > 60

Usuario: "Edad promedio de mujeres"
SQL: SELECT AVG(EDAD) FROM SALUDMENTAL WHERE SEXO = 2

Ahora genera SQL para:"""

    response = await client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_query}
        ],
        temperature=0  # Determinismo mÃ¡ximo para SQL
    )
    
    sql_query = response.choices[0].message.content.strip()
    
    # 3. ValidaciÃ³n de seguridad
    forbidden_keywords = ["DELETE", "DROP", "TRUNCATE", "UPDATE", "INSERT", "ALTER", "CREATE"]
    if any(keyword in sql_query.upper() for keyword in forbidden_keywords):
        raise SecurityError("SQL query contains forbidden operations")
    
    return sql_query
```

**EstimaciÃ³n de esfuerzo:** 3-4 dÃ­as (incluyendo testing exhaustivo)

---

## ğŸ› ï¸ ImplementaciÃ³n de Tool Calling

### Herramientas BÃ¡sicas Recomendadas

#### Tool 1: Consultar Base de Datos Oracle
```python
async def query_oracle_tool(sql_query: str, max_rows: int = 100):
    """
    Ejecuta una consulta SQL de solo lectura en Oracle ADB.
    
    Args:
        sql_query: Consulta SQL validada
        max_rows: LÃ­mite de filas a retornar
    
    Returns:
        dict con resultados formateados
    """
    try:
        with get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute(sql_query)
            
            # Obtener nombres de columnas
            columns = [desc[0] for desc in cursor.description]
            
            # Fetch con lÃ­mite
            rows = cursor.fetchmany(max_rows)
            
            # Formatear resultados
            results = [dict(zip(columns, row)) for row in rows]
            
            return {
                "success": True,
                "rows_returned": len(results),
                "columns": columns,
                "data": results
            }
    except Exception as e:
        logger.error(f"Error ejecutando SQL: {e}")
        return {
            "success": False,
            "error": str(e)
        }
```

#### Tool 2: Calcular EstadÃ­sticas Agregadas
```python
async def calculate_statistics_tool(table: str, column: str, filters: dict = None):
    """
    Calcula estadÃ­sticas descriptivas sobre una columna numÃ©rica.
    
    Args:
        table: Nombre de la tabla
        column: Columna numÃ©rica a analizar
        filters: Filtros WHERE opcionales
    
    Returns:
        dict con mean, median, std, min, max, percentiles
    """
    # Construir SQL dinÃ¡mico de forma segura
    sql = f'''
        SELECT 
            AVG("{column}") as mean,
            MEDIAN("{column}") as median,
            STDDEV("{column}") as std,
            MIN("{column}") as min,
            MAX("{column}") as max,
            PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY "{column}") as q25,
            PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY "{column}") as q75
        FROM {table}
    '''
    
    if filters:
        where_clauses = []
        for key, value in filters.items():
            if isinstance(value, str):
                where_clauses.append(f"{key} = '{value}'")
            else:
                where_clauses.append(f"{key} = {value}")
        sql += " WHERE " + " AND ".join(where_clauses)
    
    result = await query_oracle_tool(sql)
    return result
```

#### Tool 3: BÃºsqueda en Internet (Opcional - MCP)
**Solo si es necesario** para contexto adicional sobre diagnÃ³sticos:

```python
import httpx

async def search_medical_context_tool(query: str):
    """
    Busca informaciÃ³n mÃ©dica contextual en fuentes confiables.
    CUIDADO: Solo usar para definiciones generales, no diagnÃ³stico.
    """
    # Ejemplo con API de PubMed o fuente similar
    async with httpx.AsyncClient() as client:
        response = await client.get(
            "https://api.example.com/medical-search",
            params={"q": query, "limit": 3}
        )
        return response.json()
```

---

## ğŸ”Œ IntegraciÃ³n MCP (Model Context Protocol)

**Complejidad:** â­â­â­â­â­ (MUY ALTA - NO RECOMENDADO PARA HACKATHON)

MCP es un protocolo emergente de Anthropic para contexto dinÃ¡mico, pero:
- DocumentaciÃ³n limitada
- Soporte de librerÃ­as inmaduro
- Mayor overhead de desarrollo
- Beneficio marginal vs tool calling tradicional

**RecomendaciÃ³n:** Usar function calling estÃ¡ndar de OpenAI/Anthropic por ahora.

---

## ğŸ’» ImplementaciÃ³n Frontend

### Componente ChatInterface.tsx
```typescript
import { useState, useEffect, useRef } from 'react'
import type { Message } from '../types/chat'

interface ChatInterfaceProps {
  onInsightGenerated?: (insight: any) => void
}

export function ChatInterface({ onInsightGenerated }: ChatInterfaceProps) {
  const [messages, setMessages] = useState<Message[]>([])
  const [input, setInput] = useState('')
  const [loading, setLoading] = useState(false)
  const messagesEndRef = useRef<HTMLDivElement>(null)

  const sendMessage = async () => {
    if (!input.trim()) return

    const userMessage: Message = {
      role: 'user',
      content: input,
      timestamp: new Date()
    }

    setMessages(prev => [...prev, userMessage])
    setInput('')
    setLoading(true)

    try {
      const response = await fetch('/api/chat/message', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          message: input,
          conversation_id: sessionStorage.getItem('conversation_id')
        })
      })

      const data = await response.json()

      const assistantMessage: Message = {
        role: 'assistant',
        content: data.response,
        timestamp: new Date(),
        tool_calls: data.tool_calls,
        data_insights: data.data_insights
      }

      setMessages(prev => [...prev, assistantMessage])

      // Notificar insights al componente padre
      if (data.data_insights && onInsightGenerated) {
        onInsightGenerated(data.data_insights)
      }

      // Guardar conversation_id
      if (data.conversation_id) {
        sessionStorage.setItem('conversation_id', data.conversation_id)
      }
    } catch (error) {
      console.error('Error sending message:', error)
      setMessages(prev => [...prev, {
        role: 'assistant',
        content: 'Lo siento, hubo un error procesando tu solicitud.',
        timestamp: new Date(),
        error: true
      }])
    } finally {
      setLoading(false)
    }
  }

  useEffect(() => {
    messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' })
  }, [messages])

  return (
    <div className="chat-interface">
      <div className="chat-messages">
        {messages.map((msg, idx) => (
          <div key={idx} className={`message message--${msg.role}`}>
            <div className="message__content">
              {msg.content}
            </div>
            {msg.tool_calls && (
              <div className="message__tools">
                <span className="tool-badge">ğŸ”§ Herramientas usadas: {msg.tool_calls.join(', ')}</span>
              </div>
            )}
            {msg.data_insights && (
              <div className="message__insights">
                <InsightCard data={msg.data_insights} />
              </div>
            )}
          </div>
        ))}
        {loading && <div className="message message--loading">Brain estÃ¡ pensando...</div>}
        <div ref={messagesEndRef} />
      </div>

      <div className="chat-input">
        <input
          type="text"
          value={input}
          onChange={(e) => setInput(e.target.value)}
          onKeyPress={(e) => e.key === 'Enter' && sendMessage()}
          placeholder="PregÃºntame sobre los datos de salud mental..."
          disabled={loading}
        />
        <button onClick={sendMessage} disabled={loading || !input.trim()}>
          Enviar
        </button>
      </div>
    </div>
  )
}
```

---

## ğŸ“‹ Plan de ImplementaciÃ³n Recomendado

### Fase 1: Setup BÃ¡sico (DÃ­a 1-2)
- [ ] Instalar dependencias: `openai`, `numpy`, `tiktoken`
- [ ] Configurar API keys en `.env`
- [ ] Crear endpoint `/api/chat/message` en FastAPI
- [ ] Implementar gestiÃ³n bÃ¡sica de conversaciÃ³n (historial en memoria)
- [ ] Probar conexiÃ³n con LLM provider elegido

### Fase 2: RAG sobre Esquema Oracle (DÃ­a 2-3)
- [ ] Documentar esquema completo de SALUDMENTAL
- [ ] Generar embeddings del esquema
- [ ] Implementar bÃºsqueda semÃ¡ntica de contexto
- [ ] Testear recuperaciÃ³n de informaciÃ³n relevante

### Fase 3: Tool Calling (DÃ­a 3-5)
- [ ] Implementar `query_oracle_tool` con validaciÃ³n de seguridad
- [ ] Implementar `calculate_statistics_tool`
- [ ] Configurar function calling en LLM provider
- [ ] Testing exhaustivo de generaciÃ³n SQL
- [ ] Implementar fallbacks para queries errÃ³neas

### Fase 4: Frontend Integration (DÃ­a 5-6)
- [ ] Crear componente `ChatInterface.tsx`
- [ ] Integrar con backend FastAPI
- [ ] AÃ±adir visualizaciÃ³n de resultados inline
- [ ] Styling coherente con diseÃ±o Brain existente

### Fase 5: Testing & Refinamiento (DÃ­a 6-7)
- [ ] Testing end-to-end de flujos conversacionales
- [ ] OptimizaciÃ³n de prompts para contexto mÃ©dico
- [ ] Manejo de errores y edge cases
- [ ] DocumentaciÃ³n de capacidades del chatbot

### Fase 6: Deployment (DÃ­a 7-8)
- [ ] Actualizar Docker Compose con variables nuevas
- [ ] Testing en entorno productivo
- [ ] Configurar rate limiting
- [ ] Documentar uso para evaluadores

---

## ğŸ’° EstimaciÃ³n de Costes

### OpenAI GPT-4o (Modelo recomendado)
- **Input:** $2.50 / 1M tokens
- **Output:** $10.00 / 1M tokens
- **Embeddings (text-embedding-3-small):** $0.02 / 1M tokens

**Ejemplo de conversaciÃ³n tÃ­pica:**
- Pregunta usuario: ~50 tokens
- Contexto RAG: ~500 tokens
- Respuesta modelo: ~300 tokens
- **Total por interacciÃ³n:** ~850 tokens â‰ˆ **$0.01**

**Presupuesto hackathon estimado:**
- 1000 interacciones de prueba: ~$10
- 5000 interacciones demo: ~$50
- **Total:** $60-100 USD suficientes

### OpenRouter (Alternativa econÃ³mica)
- Claude 3.5 Sonnet: ~$3/$15 por 1M tokens (input/output)
- GPT-4o via OpenRouter: Similar a OpenAI
- Llama 3.1 70B: Gratis o muy barato

---

## ğŸ¯ RecomendaciÃ³n Final

### Para Malackathon (8 dÃ­as disponibles):

**Arquitectura recomendada:**
1. **LLM Provider:** OpenAI GPT-4o (estabilidad y documentaciÃ³n)
2. **RAG:** In-memory embeddings con NumPy/FAISS (simplicidad)
3. **Tools:** 2-3 herramientas bÃ¡sicas (query Oracle, stats, export)
4. **Frontend:** Componente chat integrado en pÃ¡gina existente

**Alternativa si presupuesto es crÃ­tico:**
1. **LLM Provider:** OpenRouter con Claude 3.5 Sonnet
2. Mismo stack tÃ©cnico que opciÃ³n principal
3. Coste ~30% menor

**NO recomendado para hackathon:**
- âŒ Oracle Gen AI (demasiado complejo, poca documentaciÃ³n)
- âŒ MCP (protocolo inmaduro, overhead innecesario)
- âŒ Vector DB dedicada (FAISS in-memory es suficiente)
- âŒ Fine-tuning de modelo (tiempo y coste prohibitivos)

---

## ğŸ”’ Consideraciones de Seguridad

### CrÃ­ticas para datos de salud:
1. **SQL Injection Prevention:**
   - ValidaciÃ³n estricta de SQL generado
   - Whitelist de operaciones permitidas
   - ParametrizaciÃ³n de queries

2. **Data Anonymization:**
   - NUNCA exponer CIP_SNS_RECODIFICADO completo en respuestas
   - Agregar datos antes de mostrar
   - Logging de todas las queries ejecutadas

3. **Rate Limiting:**
   ```python
   from slowapi import Limiter, _rate_limit_exceeded_handler
   from slowapi.util import get_remote_address
   
   limiter = Limiter(key_func=get_remote_address)
   app.state.limiter = limiter
   
   @app.post("/chat/message")
   @limiter.limit("30/minute")  # 30 mensajes por minuto
   async def chat_endpoint(request: Request):
       ...
   ```

4. **Audit Trail:**
   - Log de todas las interacciones con timestamp
   - Almacenar queries ejecutadas para revisiÃ³n
   - Monitoreo de patrones anÃ³malos

---

## ğŸ“š Recursos y Referencias

### DocumentaciÃ³n Clave:
- [OpenAI Function Calling](https://platform.openai.com/docs/guides/function-calling)
- [FastAPI Background Tasks](https://fastapi.tiangolo.com/tutorial/background-tasks/)
- [LangChain SQL Agent](https://python.langchain.com/docs/use_cases/sql/)
- [Oracle AI Vector Search](https://docs.oracle.com/en/database/oracle/oracle-database/23/vecse/)

### LibrerÃ­as Ãštiles:
```txt
# AÃ±adir a requirements.txt
openai==1.35.0
anthropic==0.31.0  # Si usas Claude
tiktoken==0.7.0  # Token counting
numpy==1.26.0
faiss-cpu==1.8.0  # Vector search (opcional)
slowapi==0.1.9  # Rate limiting
```

---

## âœ… Checklist de DecisiÃ³n

Antes de comenzar implementaciÃ³n, confirmar:
- [ ] LLM provider elegido y API key disponible
- [ ] Presupuesto de API aprobado (~$100 USD)
- [ ] Esquema Oracle completamente documentado
- [ ] Equipo tiene conocimientos bÃ¡sicos de async Python
- [ ] Frontend React puede extenderse con nuevo componente
- [ ] Estrategia de testing definida

---

**Preparado por:** Agente IA - Cursor  
**Para revisiÃ³n por:** Equipo DAJER - Malackathon 2025  
**PrÃ³ximo paso:** DecisiÃ³n de arquitectura y kick-off de implementaciÃ³n
